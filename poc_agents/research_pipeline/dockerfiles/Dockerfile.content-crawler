# Content Crawler Agent Dockerfile
FROM python:3.11-slim-bookworm

WORKDIR /app

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

# Install system dependencies needed for lxml
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    gcc \
    libxml2-dev \
    libxslt1-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy library and SDK first
COPY agentvault_library/ /app/agentvault_library/
COPY agentvault_server_sdk/ /app/agentvault_server_sdk/

# Copy agent code and card
COPY poc_agents/research_pipeline/content_crawler_agent.py /app/
COPY poc_agents/research_pipeline/base_agent.py /app/
COPY poc_agents/research_pipeline/agent_cards/content_crawler/agent-card.json /app/agent-card.json

# Install base dependencies (SDK/Library)
RUN pip install --no-cache-dir /app/agentvault_library /app/agentvault_server_sdk

# Install agent-specific Python dependencies
# --- ADDED: beautifulsoup4 and lxml ---
RUN pip install --no-cache-dir fastapi uvicorn httpx pydantic beautifulsoup4 lxml
# --- END ADDED ---

# Create a non-root user
RUN groupadd -r appgroup && useradd --no-log-init -r -g appgroup appuser

# Change ownership
RUN chown -R appuser:appgroup /app

# Switch user
USER appuser

# Set environment variables
ENV PORT=8011
ENV AGENT_CARD_PATH=/app/agent-card.json
# Add any other necessary ENV vars for the crawler here (e.g., API keys if needed for search)

# Expose the port
EXPOSE 8011

# Define the command
CMD ["uvicorn", "content_crawler_agent:app", "--host", "0.0.0.0", "--port", "8011", "--log-level", "debug"]
